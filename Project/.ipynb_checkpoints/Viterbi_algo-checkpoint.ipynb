{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyE1ye3kAvIW"
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries \n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uFTrD8LyAxpE",
    "outputId": "690e7297-099e-40b3-9105-b6a27e39e801"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading and importing the brown corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3kgL3RXGU4Q"
   },
   "outputs": [],
   "source": [
    "#Getting the tagged sentences\n",
    "sent_tag = brown.tagged_sents()\n",
    "mod_sent_tag=[]\n",
    "for s in sent_tag:\n",
    "  s.insert(0,('##','##'))\n",
    "  s.append(('&&','&&'))\n",
    "  mod_sent_tag.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee3csWQiG9MG"
   },
   "outputs": [],
   "source": [
    "#Splitting the data for train and test\n",
    "split_num = int(len(mod_sent_tag)*0.9)\n",
    "train_data = mod_sent_tag[0:split_num]\n",
    "test_data = mod_sent_tag[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ2P05zBJ9DC"
   },
   "outputs": [],
   "source": [
    "#Creating a dictionary whose keys are tags and values contain words which were assigned the correspoding tag\n",
    "# ex:- 'TAG':{word1: count(word1,'TAG')}\n",
    "train_word_tag = {}\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[t][w]+=1\n",
    "      except:\n",
    "        train_word_tag[t][w]=1\n",
    "    except:\n",
    "      train_word_tag[t]={w:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rqc2hLcVn3F"
   },
   "outputs": [],
   "source": [
    "#Calculating the emission probabilities using train_word_tag\n",
    "train_emission_prob={}\n",
    "for k in train_word_tag.keys():\n",
    "  train_emission_prob[k]={}\n",
    "  count = sum(train_word_tag[k].values())\n",
    "  for k2 in train_word_tag[k].keys():\n",
    "    train_emission_prob[k][k2]=train_word_tag[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCP3CF2IZn-T"
   },
   "outputs": [],
   "source": [
    "#Estimating the bigram of tags to be used for transition probability\n",
    "bigram_tag_data = {}\n",
    "for s in train_data:\n",
    "  bi=list(nltk.bigrams(s))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pLyInPvZpN8"
   },
   "outputs": [],
   "source": [
    "#Calculating the probabilities of tag bigrams for transition probability  \n",
    "bigram_tag_prob={}\n",
    "for k in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[k]={}\n",
    "  count=sum(bigram_tag_data[k].values())\n",
    "  for k2 in bigram_tag_data[k].keys():\n",
    "    bigram_tag_prob[k][k2]=bigram_tag_data[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9vGF9AMlZwB"
   },
   "outputs": [],
   "source": [
    "#Calculating the possible tags for each word\n",
    "#Note: Here we have used the whole data(Train+Test)\n",
    "#Reason: There may be some words which are not present in train data but are present in test data \n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "        \n",
    "for s in test_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-ilX9pXdBTt"
   },
   "outputs": [],
   "source": [
    "#Dividing the test data into test words and test tags\n",
    "test_words=[]\n",
    "test_tags=[]\n",
    "for s in test_data:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    temp_word.append(w.lower())\n",
    "    temp_tag.append(t)\n",
    "  test_words.append(temp_word)\n",
    "  test_tags.append(temp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d90XCziOm7EH"
   },
   "outputs": [],
   "source": [
    "#Executing the Viterbi Algorithm\n",
    "predicted_tags = []                #intializing the predicted tags\n",
    "for x in range(len(test_words)):   # for each tokenized sentence in the test data\n",
    "  s = test_words[x]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "  storing_values = {}              \n",
    "  for q in range(len(s)):\n",
    "    step = s[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:                \n",
    "      storing_values[q] = {}\n",
    "      tags = tags_of_tokens[step]\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['##',bigram_tag_prob['##'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['##',0.0001]#*train_emission_prob[t][step]]\n",
    "    \n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:                             \n",
    "        temp = []\n",
    "        for pt in previous_states:                         \n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step])\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)]\n",
    "\n",
    "  #Backtracing to extract the best possible tags for the sentence\n",
    "  pred_tags = []\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)\n",
    "  for bs in range(len(total_steps_num)):\n",
    "    step_num = last_step_num - bs\n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('&&')\n",
    "      pred_tags.append(storing_values[step_num]['&&'][0])\n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
    "  predicted_tags.append(list(reversed(pred_tags)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "L9CBrphuEu0J",
    "outputId": "340acac0-2302-4293-eaa2-dc86061d891d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data is:  0.9214105322233654\n",
      "Loss on the test data is:  0.07858946777663456\n"
     ]
    }
   ],
   "source": [
    "#Calculating the accuracy based on tagging each word in the test data.\n",
    "right = 0 \n",
    "wrong = 0\n",
    "for i in range(len(test_tags)):\n",
    "  gt = test_tags[i]\n",
    "  pred = predicted_tags[i]\n",
    "  for h in range(len(gt)):\n",
    "    if gt[h] == pred[h]:\n",
    "      right = right+1\n",
    "    else:\n",
    "      wrong = wrong +1 \n",
    "\n",
    "print('Accuracy on the test data is: ',right/(right+wrong))\n",
    "print('Loss on the test data is: ',wrong/(right+wrong))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Viterbi-Algo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
